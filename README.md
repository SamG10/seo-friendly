# ğŸ”§ **Exercice 1 â€“ CrÃ©er un Mini-Site HTML SEO-Friendly**

### ğŸ¯ Objectif :

Concevoir un mini-site web statique, structurÃ© de maniÃ¨re Ã  Ãªtre **explorable par un moteur de recherche**, et **rÃ©fÃ©rencement-friendly**. Ce site servira ensuite de base pour Ãªtre analysÃ© par un mini-crawler que vous dÃ©velopperez dans un second temps.

---

### ğŸ§± Contraintes techniques du mini-site :

- Le site doit Ãªtre en **HTML/CSS pur** (pas de framework).
- Il doit contenir entre **5 et 7 pages internes**, liÃ©es entre elles.
- Toutes les pages doivent Ãªtre **accessibles depuis la page dâ€™accueil**.
- Inclure une **navigation principale** (header/menu).
- Utiliser une **structure sÃ©mantique HTML5 correcte** :
  - `<header>`, `<main>`, `<footer>`, `<nav>`, `<article>`, etc.
- Ajouter des **balises SEO importantes** :
  - `<title>`, `<meta name="description">`
  - Une seule balise `<h1>` par page
  - Des images avec `alt`

---

### ğŸ’¡ Contenu recommandÃ© :

- Une page dâ€™accueil (`index.html`)
- Une page "Ã€ propos"
- Une page "Nos services"
- Une page "Contact"
- Une ou deux pages de blog ou articles

---

### âš ï¸ PiÃ¨ges SEO Ã  intÃ©grer volontairement :

Pour prÃ©parer le travail du mini-crawler, vous devez volontairement insÃ©rer :

- Au moins **1 lien cassÃ©** (`href="page-qui-nexiste-pas.html"`)
- Une page avec **balise `<title>` manquante**
- Une page sans `<meta description>`
- Un lien **vers un site externe**

---

### ğŸ—‚ï¸ Organisation recommandÃ©e :

```
/mon-mini-site
  â”œâ”€â”€ index.html
  â”œâ”€â”€ about.html
  â”œâ”€â”€ services.html
  â”œâ”€â”€ contact.html
  â”œâ”€â”€ blog/
  â”‚   â”œâ”€â”€ article1.html
  â”‚   â””â”€â”€ article2.html
  â””â”€â”€ css/
      â””â”€â”€ style.css
```

---

Une fois le site prÃªt, vous pourrez lâ€™ouvrir localement avec un serveur simple :

```bash
npx http-server ./mon-mini-site
```

---

# ğŸ§ª **TP 2 â€“ DÃ©veloppement dâ€™un Mini-Crawler Node.js + Sitemap**

### ğŸ¯ Objectif :

CrÃ©er un **mini crawler web** en Node.js capable dâ€™explorer automatiquement votre mini-site local, dâ€™identifier les pages internes accessibles, et de gÃ©nÃ©rer un **fichier sitemap.xml** dynamique pour le rÃ©fÃ©rencement.

> Ce TP se fait en **travail individuel**.

---

### ğŸ› ï¸ FonctionnalitÃ©s attendues du script :

1. **Explorer rÃ©cursivement les pages internes**

   - DÃ©marrer depuis une URL de base (ex : `http://localhost:8080`)
   - Suivre les liens internes (`<a href>` avec des chemins relatifs)
   - **Ignorer les liens externes, ancres, fichiers (.jpg, .pdf, .zip, etc.)**
   - **Ã‰viter les boucles** et les pages dÃ©jÃ  visitÃ©es

2. **GÃ©nÃ©rer un `sitemap.xml`**
   - Respecter la structure standard :
     ```xml
     <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
       <url>
         <loc>http://localhost:8080/page.html</loc>
         <changefreq>weekly</changefreq>
         <priority>0.8</priority>
       </url>
     </urlset>
     ```
   - Le fichier doit inclure **uniquement les pages valides** trouvÃ©es pendant le crawl
   - Enregistrer le fichier Ã  la racine du projet

---

### ğŸ’» Structure du projet recommandÃ©e :

```
/mini-crawler
  â”œâ”€â”€ crawler.js
  â”œâ”€â”€ sitemap.xml
  â””â”€â”€ package.json
```

---

### ğŸ’¡ Aide au dÃ©marrage :

#### ğŸ“¦ Installation des dÃ©pendances :

```bash
npm init -y
npm install axios cheerio xml minimist
```

#### ğŸ” Algorithme de base :

1. CrÃ©er une `Set` pour suivre les pages dÃ©jÃ  visitÃ©es
2. Utiliser `axios` pour charger chaque page
3. Parser les balises `<a>` avec `cheerio`
4. Filtrer les URLs valides internes
5. Appeler rÃ©cursivement la fonction `crawl()` pour chaque lien trouvÃ©

---

ğŸ§ª Exemple de commande :

```bash
node crawler.js --url=http://localhost:8080 --maxDepth=2
```

---

ğŸ“£ **Conseil** : testez votre `sitemap.xml` ici â†’  
ğŸ‘‰ [https://www.xml-sitemaps.com/validate-xml-sitemap.html](https://www.xml-sitemaps.com/validate-xml-sitemap.html)

## ğŸ“ˆ Liste des optimisations rÃ©alisÃ©es

### Optimisations SEO

- Utilisation de balises sÃ©mantiques HTML5 (`<header>`, `<main>`, `<footer>`, `<nav>`, `<article>`, `<section>`)
- Structure hiÃ©rarchique des titres correcte (h1, h2, h3)
- Balises meta description prÃ©sentes sur les pages principales
- Attributs alt descriptifs sur les images
- URLs propres et descriptives
- Navigation claire et accessible

### Optimisations Performance

- Utilisation de `preconnect` pour les polices Google Fonts
- Chargement diffÃ©rÃ© des images avec `loading="lazy"`
- CSS optimisÃ© et minifiÃ©
- Utilisation de variables CSS pour une maintenance facilitÃ©e
- Images au format WebP pour un meilleur rendement
- Design responsive avec media queries

### Optimisations UX/UI

- Interface utilisateur moderne et Ã©purÃ©e
- Animations subtiles pour amÃ©liorer l'interaction
- SystÃ¨me de grille flexible pour les articles
- Design responsive adaptÃ© aux mobiles
- Contraste des couleurs optimisÃ© pour l'accessibilitÃ©
- Transitions fluides pour les interactions

### Optimisations Code

- Structure de projet claire et organisÃ©e
- Code HTML bien indentÃ© et commentÃ©
- Utilisation de classes CSS rÃ©utilisables
- SystÃ¨me de design cohÃ©rent avec variables CSS
- Gestion des Ã©tats hover et focus
- Support des navigateurs modernes

## ğŸ–¼ï¸ Liens des images

### Rapport initial Lighthouse (Desktop)

- ![Initial desktop](./assets/initialD.png)

### Rapport initial Lighthouse (Mobile)

- ![Initial mobile](./assets/initialM.png)

### Rapport aprÃ¨s optimisation du code (Desktop)

- ![OptimisÃ© desktop](./assets/optimiseD.png)

### Rapport aprÃ¨s optimisation du code (Mobile)

- ![OptimisÃ© mobile](./assets/optimiseM.png)
