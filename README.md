# ğŸ”§ **Exercice 1 â€“ CrÃ©er un Mini-Site HTML SEO-Friendly**

### ğŸ¯ Objectif :

Concevoir un mini-site web statique, structurÃ© de maniÃ¨re Ã  Ãªtre **explorable par un moteur de recherche**, et **rÃ©fÃ©rencement-friendly**. Ce site servira ensuite de base pour Ãªtre analysÃ© par un mini-crawler que vous dÃ©velopperez dans un second temps.

---

### ğŸ§± Contraintes techniques du mini-site :

- Le site doit Ãªtre en **HTML/CSS pur** (pas de framework).
- Il doit contenir entre **5 et 7 pages internes**, liÃ©es entre elles.
- Toutes les pages doivent Ãªtre **accessibles depuis la page dâ€™accueil**.
- Inclure une **navigation principale** (header/menu).
- Utiliser une **structure sÃ©mantique HTML5 correcte** :
  - `<header>`, `<main>`, `<footer>`, `<nav>`, `<article>`, etc.
- Ajouter des **balises SEO importantes** :
  - `<title>`, `<meta name="description">`
  - Une seule balise `<h1>` par page
  - Des images avec `alt`

---

### ğŸ’¡ Contenu recommandÃ© :

- Une page dâ€™accueil (`index.html`)
- Une page â€œÃ€ proposâ€
- Une page â€œNos servicesâ€
- Une page â€œContactâ€
- Une ou deux pages de blog ou articles

---

### âš ï¸ PiÃ¨ges SEO Ã  intÃ©grer volontairement :

Pour prÃ©parer le travail du mini-crawler, vous devez volontairement insÃ©rer :

- Au moins **1 lien cassÃ©** (`href="page-qui-nexiste-pas.html"`)
- Une page avec **balise `<title>` manquante**
- Une page sans `<meta description>`
- Un lien **vers un site externe**

---

### ğŸ—‚ï¸ Organisation recommandÃ©e :

```
/mon-mini-site
  â”œâ”€â”€ index.html
  â”œâ”€â”€ about.html
  â”œâ”€â”€ services.html
  â”œâ”€â”€ contact.html
  â”œâ”€â”€ blog/
  â”‚   â”œâ”€â”€ article1.html
  â”‚   â””â”€â”€ article2.html
  â””â”€â”€ css/
      â””â”€â”€ style.css
```

---

Une fois le site prÃªt, vous pourrez lâ€™ouvrir localement avec un serveur simple :

```bash
npx http-server ./mon-mini-site
```

---

# ğŸ§ª **TP 2 â€“ DÃ©veloppement dâ€™un Mini-Crawler Node.js + Sitemap**

### ğŸ¯ Objectif :

CrÃ©er un **mini crawler web** en Node.js capable dâ€™explorer automatiquement votre mini-site local, dâ€™identifier les pages internes accessibles, et de gÃ©nÃ©rer un **fichier sitemap.xml** dynamique pour le rÃ©fÃ©rencement.

> Ce TP se fait en **travail individuel**.

---

### ğŸ› ï¸ FonctionnalitÃ©s attendues du script :

1. **Explorer rÃ©cursivement les pages internes**

   - DÃ©marrer depuis une URL de base (ex : `http://localhost:8080`)
   - Suivre les liens internes (`<a href>` avec des chemins relatifs)
   - **Ignorer les liens externes, ancres, fichiers (.jpg, .pdf, .zip, etc.)**
   - **Ã‰viter les boucles** et les pages dÃ©jÃ  visitÃ©es

2. **GÃ©nÃ©rer un `sitemap.xml`**
   - Respecter la structure standard :
     ```xml
     <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
       <url>
         <loc>http://localhost:8080/page.html</loc>
         <changefreq>weekly</changefreq>
         <priority>0.8</priority>
       </url>
     </urlset>
     ```
   - Le fichier doit inclure **uniquement les pages valides** trouvÃ©es pendant le crawl
   - Enregistrer le fichier Ã  la racine du projet

---

### ğŸ’» Structure du projet recommandÃ©e :

```
/mini-crawler
  â”œâ”€â”€ crawler.js
  â”œâ”€â”€ sitemap.xml
  â””â”€â”€ package.json
```

---

### ğŸ’¡ Aide au dÃ©marrage :

#### ğŸ“¦ Installation des dÃ©pendances :

```bash
npm init -y
npm install axios cheerio xml minimist
```

#### ğŸ” Algorithme de base :

1. CrÃ©er une `Set` pour suivre les pages dÃ©jÃ  visitÃ©es
2. Utiliser `axios` pour charger chaque page
3. Parser les balises `<a>` avec `cheerio`
4. Filtrer les URLs valides internes
5. Appeler rÃ©cursivement la fonction `crawl()` pour chaque lien trouvÃ©

---

ğŸ§ª Exemple de commande :

```bash
node crawler.js --url=http://localhost:8080 --maxDepth=2
```

---

ğŸ“£ **Conseil** : testez votre `sitemap.xml` ici â†’  
ğŸ‘‰ [https://www.xml-sitemaps.com/validate-xml-sitemap.html](https://www.xml-sitemaps.com/validate-xml-sitemap.html)
